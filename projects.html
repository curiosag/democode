<!DOCTYPE html>
<html>

<head>
<meta charset='utf-8'>
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="description" content="Projects">

<link rel="stylesheet" type="text/css" media="screen"
	href="stylesheets/stylesheet.css">

<title>Projects</title>

<script type="text/javascript">
	//<![CDATA[
	​
	//]]>
</script>

</head>

<body onload="init()">

	<!-- HEADER -->
	<div id="header_wrap" class="outer">
		<header class="inner">

			<h1 id="project_title">Projects</h1>
			<h2 id="project_tagline"></h2>

		</header>
	</div>

	<!-- MAIN CONTENT -->
	<div id="main_content_wrap" class="outer">
		<section id="main_content" class="inner">

			A selection of projects with different application domains,
			technologies and employers/customers. Some of them also come up in a
			<a href="./index.html">related collection of code samples</a>.

			<h3>Financial Data-Feed Processing</h3>
			A series of individual projects spread out over a couple of years,
			implementing components for processing financial data-feeds of
			different providers. Implementations were usually internationally
			outsourced.<br> <br>Analysis involved identification and
			verification of sources for data, meta-data and descriptive data,
			in-depth resolution of gaps and ambiguities, and eventually model
			mapping from the provider model to the internally used meta-data.
			Meta-data meant definitions for the structure of data, descriptive
			data gave meaningful names to elements of meta-data and codes, e.g.
			to country-codes. <br> <br>Each component covered
			extraction and update for meta-data and descriptive data, if
			supported by the provider, provider specific data requesting- and
			delivery logic, raw data parsing, evaluation, cleansing and
			eventually storage to a repository, considering provider-defined data
			mutation rules. Raw data processing was meta-data driven.<br> <br>
			There were implementations for bulk- and selection oriented feeds,
			file and api based request protocols and delivery channels, the
			latter coming in batch- and real-time flavours, and one sample of a
			request format, that came in form of a sequence of queries close to
			Sql.<br> <br>Providers covered were Bloomberg Backoffice
			and Per Security, Reuters RMDS, RDB, Equities and Fixed Income and
			Swift Iso15022. In a consulting role I dealt with
			Wertpapiermanagement, Ftid, Reuters Select and general XML-feed
			processing. <br> <br>The components were integral part of a
			standard product.
			<p>
				<code>technology</code>
				: Delphi
			</p>
			<p>
				<code>roles</code>
				: consulting, analysis, design, technical project management,
				integration, test, 3rd level support, maintenance
			</p>
			<h3>Generic Data Persistence and Manipulation Framework</h3>
			</p>
			</p>
			Implementation of given abstract classes: Meta-data driven framework
			for loading and updating data. Single data items could be trees of
			1:n-related records, covering flat structures as its simplest case.
			Data sinks were relational databases (Oracle, DB2 and DbIsam
			natively, Microsoft SqlServer, Sybase and Postgres via generic
			Borland Database Engine) and XML output format. For native access the
			implementation supported automatic database structure changes upon
			changed meta-data, for DB2 and Oracle the the DML-array interface was
			supported. <br> <br>A byproduct was generic, database
			independent table creation, table and index structure change,
			cursor-oriented table data manipulation and Sql query execution.<br>
			<br>Element of a standard product.
			</p>
			<p>
				<code>technology</code>
				: Delphi, BDE, Oracle, DB2, DbIsam, SqlServer, Postgres, Xml
			</p>
			<p>
				<code>roles</code>
				: detail design, database design, implementation, 3rd level support,
				maintenance
			</p>
			<h3>Client-Server Framework for Remote Data Processing</h3>
			<p>Splitting of an existing monolithic data processing platform
				(that contained the previous project btw.) into a client part that
				defines, initiates and monitors sequences of data processing either
				by a monitoring console or by an api and a server part, responsible
				for execution and co-ordination of multiple simultaneous client
				requests. The server part can optionally run in-process, maintaining
				the previous monolithic mode. The implementation was built upon a
				pre-existing abstraction layer upon middleware.</p>
			<p>
				<code>technology</code>
				: Delphi
			</p>
			<p>
				<code>Team Size</code>
				: 4
			</p>
			<p>
				<code>roles</code>
				: TPM, design, implementation
			</p>
			<h3>.Net Re-implementation of core data processing functionality</h3>
			Re-implementation and extension of processing elements for financial
			data providers in a .Net data processing framework with client-server
			architecture. Encompassed functionality for request- and
			delivery-handling including request pooling <small><sup><a
					href="#ref1">1</a></sup></small>, data extraction, sanitation and normalization.
			Interface definition and preliminary implementation for a related
			staging area. Implementation of a provider server emulation framework
			for automated testing.<br> <br>Elements of a standard
			product.
			<p>
				<code>technology</code>
				: C#, RavenDb
			</p>
			<p>
				<code>roles</code>
				: technical consultant, grass-root activist, implementation, test
			</p>
			<h3>Rating Data Processing</h3>
			Framework for processing of rating data. The initial project was for
			Clearstream Luxemburg, Standard & Poor's ratings with follow up
			projects for Moody's and Fitch. The component became part of the
			standard release.<br> <br></>A common, provider independent
			chain of processing steps, fitting in provider specific operations
			where needed, like Oracle loader files or transformation views.
			Common synchronization of master data with pre-loaded, sanitized
			ratings. The design rendered the follow-up projects rather
			economical. There was some architectural reuse possible for WSS from
			Deutsche Börse and IDC prices.<br> <br>Initially customer
			projects, the implementation became part of a standard product.
			<p>
				<code>technology</code>
				: Oracle 10g, Oracle loader, Sql, Pl/Sql, proprietary scripting
				language
				</code>
			</p>
			<p>
				<code>roles</code>
				: design, implementation, 3rd level support, maintenance
			</p>

			<h3>Elements of a Master Data Management System</h3>
			Generic representation and processing of complex decision rules,
			implementation of table- and message-oriented data in- and outflow
			channels covering business-, mapping- and sanitation rules.
			<p>
				<code>technology</code>
				: Oracle 10g
			</p>
			<p>
				<code>roles</code>
				: design, implementation
			</p>
			<h3>Integrated Project- and Document Management System</h3>
			A general integrated project management toolset for Primas
			Consulting, Vienna, able to cover arbitrary models, pre-configured
			with the project management model of the client. Integration of the
			involved Microsoft tools on the api level.
			<p>
				<code>technology</code>
				: MsAccess, MsExcel, MsProject, MsWord, VBA
			</p>
			<p>
				<code>Team Size</code>
				: 3
			</p>
			<p>
				<code>roles</code>
				: analysis, design, implementation, project management
			</p>
			<h3>Hotel Resource Allocation and Reservation System</h3>
			Multi-dimensional resource allocation and billing system in the hotel
			business domain (with resources like tennis courts, golf tracks or
			instructurs) with client/server architecture. Consulting role for the
			resource allocation algorithm. <br> <br>Customer project.
			<p>
				<code>technology</code>
				: Java, UML
			</p>
			<p>
				<code>Team Size</code>
				: 5
			</p>
			<p>
				<code>roles</code>
				: consultant, QA
			</p>

			<h3>Xml storage prototype and evaluation</h3>
			Implementation of different Xml storage models (Binary Xml, XMLType,
			CLOB) with Oracle, combined with parallelized Xslt processing and
			storage. Comparison for high volume data with an existing relational
			single thread implementation, benchmarks, derivation of implications
			for possible implementations.
			<p>
				<code>technology</code>
				: C#, Oracle 10g, Oracle XML Db
			</p>
			<p>
				<code>roles</code>
				: analysis, implementation, evaluation
			</p>
			<h3>Pl/Sql rule compiler</h3>
			Extension of an existing rule engine, that worked as an interpreter
			by code generation functionality, rendering it to a rule compiler.
			<p>
				<code>technology</code>
				: Oracle 10g, Pl/Sql
			</p>
			<p>
				<code>roles</code>
				: analysis, implementation
			</p>
			<h3>Model based data extraction from Telekurs data</h3>
			Proof of concept for model generation from Six Telekurs meta-data
			sources and data extractions from the object model, covering existing
			Sql queries on Telekurs data. The final model chosen for the product
			was close to the prototype.
			<p>
				<code>technology</code>
				: C#
			</p>
			<p>
				<code>roles</code>
				: analysis, implementation
			</p>

			<p>
				<br> <br>
			<hr>
			<a name="ref1"></a><small>1 Request pooling was an
				interesting feature to minimize the cost of requested data.
				Instruments, e.g. a number of ISINs, that were the same in different
				request, were removed from them and processed separately in another
				one. The resulting data data was merged with the resulting data of
				the residual original requests. This strategy only works for some
				payment models, though. For others the opposite is necessary:
				squeeze as many requests as possible into one.</small>
			</p>
		</section>
	</div>


	<!-- FOOTER  -->
	<div id="footer_wrap" class="outer">
		<footer class="inner">
			<p class="copyright">
				Demo Code maintained by <a href="https://github.com/curiosag">curiosag</a>
			</p>
			<p>
				Published with <a href="https://pages.github.com">GitHub Pages</a>
			</p>
		</footer>
	</div>


</body>
</html>
